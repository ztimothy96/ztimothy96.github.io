---
layout: post
title: "Designing YouTube"
subtitle: "System design interviews, chapter 14"
thumbnail-img: /assets/img/system-design/swe.png
share-img: /assets/img/system-design/swe.png
tags: [software engineering]
---
I'm re-reading *System Design Interview: An Insider's Guide* by Alex Xu. Here's Chapter 14 on how to design YouTube.

## Understand the problem

**Functional requirements**:
- *Upload*: publish a video on the site.
- *Watch*: search for and view existing videos.

Here are some details about these workflows.
- *Platforms*: support mobile app, web browsers, TVs.
- *Video format*: should display video in various resolutions and formats.
- *Video size*: maximum of 1 GB. Assume average of 300 MB.
- *Geography*: support international users.
- *Third-party services*: can use existing cloud infrastructure. 

**Non-functional requirements**:
- *Highly scalable*: should support 5 million daily active users.
- *Highly available*: watch anywhere, anytime.
- *Highly reliable*: viewing should not be frequently interrupted by buffering, for example.
- *Low latency*: viewers can upload videos quickly and stream videos smoothly.
- *Low infrastructure cost*: reduce service from third-party APIs, e.g. cloud CDNs.

## High-level design

There are two main functional requirements for the YouTube system: uploading and streaming videos.

Streaming is simple enough: we want to get the video directly from a CDN. When we stream videos, we download little bits and pieces throughout the viewing experience, so that the user can start watching right away without waiting for the whole video to download. Chunking the video also allows us to change format halfway through, for instance if the internet connection worsens and we need a lower resolution. So our goal for uploading videos will be to process videos into small chunks in multiple formats and store them in the CDN.

For uploading videos, we start with a vanilla architecture. However, here the users are trying to upload large video files, which are not handled by HTTP. Instead, they will use the File Transport Protocol (FTP), so we should make a new component which uses FTP and simply awaits file uploads. After the files are uploaded, we will need to transcode them into different formats, store those transcoded videos in the CDN, store metadata about the videos, and send notifications to users that their upload is complete. Once again, we separate concerns, creating new components for each of these steps. The result is shown in the figure below.

![14-5](/assets/img/system-design/14-5.png){: .mx-auto.d-block :}

Updating metadata is fairly straightforward: the user can send an HTTP request to the API servers, which update the metadata databases.

## Detailed design

In the deep dive, Xu talks about video uploading, video streaming, and error handling.

### Uploading and transcoding

Need to perform many parallel and expensive video processing tasks. We may need to save videos at different resolutions, add watermarks, or detect thumbnails. We will also need a system that can flexibly select a few of these tasks to perform; for example, some users upload high-definition videos and create their own thumbnails, so we can skip upsampling or thumbnail generation for them. To separate concerns, we we can put all our tasks into a DAG: that way, we know which options we can select and which order we should perform them.

![14-15](/assets/img/system-design/14-15.png){: .mx-auto.d-block :}

The way we can use the DAG is the following: first, we should preprocess a video by splitting it into chunks for streaming. Then we can assign tasks to each chunk from the DAG and put them in a queue. After that, we have to find an idle server and assign some queued tasks to it. Once all the tasks are completed, we can stitch the chunks back together to form a transcoded video. Again, we can separate concerns by making a new component to perform each step above.

![14-10](/assets/img/system-design/14-10.png){: .mx-auto.d-block :}

Here is a diagram showing how the resource manager component might assign tasks to workers by pulling everything from queues. (Mumble something here about Factorio assembly machines pulling ingredients from multiple transport belts.)

![14-17](/assets/img/system-design/14-17.png){: .mx-auto.d-block :}

### Performance optimizations
Queues are the glue that hold systems together. Just as we used queues to feed transcoding tasks to workers in parallel, we can join components along the entire video uploading workflow to introduce greater parallelism. This allows downstream components to work asynchronously, without waiting for new inputs from the upstream components.

![14-26](/assets/img/system-design/14-26.png){: .mx-auto.d-block :}

We can create multiple upload centers around the world and let the users upload videos to the geographically nearest one. This reduces network latency. But maybe this idea was pretty obvious.

Ideally, we'd like to store everything in a CDN so that users can stream videos quickly. However, CDN storage is expensive, so we should consider the trade-offs between performance and operating cost. The best solution is hybrid as usual: we can store only the most popular videos in a CDN and leave the less commonly requested ones in normal servers. Videos that are popular only in a certain region of the world can be store in CDNs for that region only (for instance, Kdramas might go in an Asian CDN cluster). Some videos are short enough that we can skip transcoding them and instead encode them when requested.

Xu also includes a section on how to handle errors on the system. However, I find this to be rather unhelpful. It really only highlights two ideas: "replicate servers everywhere" or "retry tasks when they fail." (Hopefully some information about the task was stored in a queue or database somewhere.)

## Look back

Here are some key takeaways from this design session. This is kind of predictable...
- *Separate your workflows.* Here there are two functional requirements: uploading and streaming videos. We spent much more time looking at the uploading workflow.
- *Separate your concerns.* Create a new system component for each step in your algorithm. We used this idea to design the video uploading system and the video transcoding sub-system inside it.
- *Queues are glue.* Again, queues are used in almost all cases where we want to pass messages asynchronously. We spammed queues everywhere in both the video uploading and video transcoding workflows.
- *Hybrid is best.* We used a mixture of CDN and regular video storage to achieve good trade-offs between system performance and operating cost.

I think I am getting bored of this book.
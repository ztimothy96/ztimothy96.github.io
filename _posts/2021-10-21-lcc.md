---
layout: post
title: "Locally correctable codes"
subtitle: "Algebraic coding theory, part 5"
thumbnail-img: /assets/img/message.png
share-img: /assets/img/message.png
tags: [math, coding theory, algebra]
---

This is part of a series in which I work through Prof. Mary Wootters' Youtube [lectures](https://www.youtube.com/watch?v=vfjN7MmSB6g&list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr) on algebraic coding theory. This post is about how to correct errors in a codeword when we only care about part of the codeword.

## What are LCC's?
Locally correctable codes, and other variants of "local" codes, can help us store data. Imagine that Alice sends a large amount of data to Bob, who stores it on a distributed cluster of computers. Then one day a cosmic ray descends from outer space and corrupts some of the data on Bob's computers. Meanwhile, Alice asks to retrieve a small amount of the data, say the $$i$$-th bit. Bob needs to recover this bit.

In this scenario, we will model the error by saying that a constant proportion of the bits can be corrupted. Let's called this the *relative distance* between the true and corrupted codewords.

>**Definition.** The relative distance between two words $$x, y$$ of length $$n$$ is their normalized Hamming distance, $$\delta(x, y) = \Delta(x, y) / n$$.

Bob could simply retrieve all of the stored data, recover Alice's original message using a standard error correction algorithm, and then encode it again. However, this is very wasteful when the length $$n$$ is large. If we only want to retrieve a single bit, instead of the entire codeword, we might want to use less information and do less computation. This is the idea behind locally correctable codes.

Let's think about what it means to succeed at local decoding. Suppose that a codeword $$c$$ is corrupted to some word $$w$$ with $$\delta(w, c) < p$$. Given access to $$w$$, and given a bit $$i$$ requested by Alice, Bob would like an algorithm $$B(w, i)$$ which performs only a few queries and returns $$c_i$$.

Unfortunately, this task is impossible to do deterministically. To see this, suppose Bob only makes $$Q$$ queries, where $$Q < \delta n$$. The adversary Eve can run Bob's algorithm and figure out which bits Bob will query, and then she can corrupt as many as she likes, preventing Bob from getting any useful information out of his queries. To avoid being outfoxed by Eve in this way, Bob must use a random strategy. This observation motivates the following definition.

>**Definition.** A code $$C$$ is a $$(p, Q, \gamma)$$-locally correctable code (LCC) if there is an algorithm $$B$$ which can quickly correct any slightly corrupted word $$w$$ at index $$i$$. That is, given oracle access to $$w$$ with $$\delta(w, c) < p$$ for some codeword $$c$$, $$B$$ can return $$c_i$$ with $$Q$$ queries correctly with probability $$1-\gamma$$.

## Hadamard codes

First we will see a classical family of codes, the Hadamard codes, with very large distance. These codes are an extreme example of LCC's. In what follows, we'll denote inner products as $$\langle f, g \rangle = \sum f_i \cdot g_i $$, even though inner products don't have quite the same usual geometric meaning when working in a finite field.

>**Definition.** The Hadamard code of length $$2^m$$ is $$H(m) = \{ (f(\alpha_1), ..., f(\alpha_{2^m})): f(\alpha) = \langle f, \alpha \rangle, \alpha_i \in \mathbb{F}_ {2^m} \}$$.

This definition is quite a mouthful, so let's try to parse it carefully. Recall that $$\mathbb{F}_ {2^m} $$ is a finite field which is also a $$m$$-dimensional vector field over $$\mathbb{F}_2$$. The *messages* of the Hadamard code are vectors $$f \in \mathbb{F}_ 2^m$$. The *codewords* are evaluated inner products of a message with every vector in $$\mathbb{F}_{2^m}$$.

Hadamard codes have terrible rate, but very good distance. Since the messages are vectors of length $$m$$, and the codewords are $$2^m$$ evaluations, the communication rate is $$m/2^m$$. A little reflection shows that the distance is $$2^m / 2$$.

>**Proof.** Hadamard codes are linear, so the distance is also the minimum weight. But if $$f \neq 0$$, then exactly half the vectors will have dot product $$1$$ and half will have dot product $$0$$. Therefore, the minimum weight of the code is $$2^m / 2$$. $$\square$$

Let's think about how to locally decode Hadamard codes with only $$2$$ queries. Suppose the true message is $$f$$. We're given query access to the corrupted codeword, which is a function $$g$$ which is slightly corrputed from $$f$$. That is, $$f(\alpha) = g(\alpha)$$ on all except $$p \cdot 2^m$$ points $$\alpha \in \mathbb{F}_{2^m}$$. We want to find a single letter of the codeword, $$f(\alpha)$$ for a given point $$\alpha$$.

Here's a simple algorithm for local corrections. We'll make several queries, each of which seems uniformly random on its own. That way, Eve doesn't know which bits to corrupt. However, the queries will be correlated, so they can provide useful information that allows us to decode a particular bit.

>**Algorithm.** Randomly choose $$\beta \in \mathbb{F}_{2^m}$$. Return $$g(\beta) + g(\beta + \alpha)$$.

This algorithm clearly only makes two queries at $$\beta$$ and $$\beta + \alpha$$. It has failure probability $$2p$$, which is better than randomly guessing the bit value when $$p < 1/4$$.

>**Proof.** If we pick $$\beta$$ randomly, then $$\Pr(f(\beta) \neq g(\beta)) \leq p$$. Similarly, $$\Pr(f(\beta + \alpha) \neq g(\beta + \alpha)) \leq p$$. With probability $$1-2p$$, we are lucky and $$f(\beta) = g(\beta), f(\beta + \alpha) = g(\beta + \alpha)$$. Then $$g(\beta) + g(\beta + \alpha) = f(\beta) + f(\beta + \alpha) =  \langle f, \beta \rangle + \langle f, \beta + \alpha \rangle = \langle f, \alpha \rangle = f(\alpha)$$. $$\square$$

That's all well and good, but the rate of Hadamard codes is truly terrible. Can we get LCC's with better rate?

## Locally decoding Reed-Muller codes.

Yes, better codes exist! Consider Reed-Muller codes, which are multi-variate generalizations of Reed-Solomon codes. Their messages are multivariate polynomials on $$m$$ variables of degree $$< r$$. Meanwhile, their codewords are evaluations of those polynomials on $$\mathbb{F}_{q^m}$$.

Again, we are given query access to corrupted codeword, which is a multivariate polynomial $$g$$. We want to find a letter of the true codeword $$c$$, which is an evaluation at a point $$f(\alpha): \alpha \in \mathbb{F}_{q^m}$$.

It's a bit beyond the scope of this post to examine Reed-Muller codes in depth, but I'll just note here that the ideas are very similar to those for Hadamard codes: pick individually random, but collectively correlated queries.

>**Algorithm (sketch).** Pick a random line $$\ell$$ through the desired point $$\alpha$$ and consider the restriction of $$f$$ to $$\ell$$. This is a single-variable polynomial of degree at most $$r$$. Query the entire line; we want to correct errors on the line, but that's the same as just correcting a RS code. $$\square$$

Doing some computation, it turns out that for a constant-rate code, we can locally decode using $$n^{1/2}$$ queries. With that in mind, we can ask questions about other tradeoffs between query, distance and rate. When we want to make $$Q = \log n$$ queries, RM codes are the best we know how to do, but getting better results is an open problem!

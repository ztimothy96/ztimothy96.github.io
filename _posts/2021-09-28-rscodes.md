---
layout: post
title: "Reed-Solomon codes"
subtitle: "Algebraic coding theory, part 3"
thumbnail-img: /assets/img/message.png
share-img: /assets/img/message.png
tags: [math, coding theory, algebra]
---

This is part of a series in which I work through Prof. Mary Wootters' Youtube [lectures](https://www.youtube.com/watch?v=vfjN7MmSB6g&list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr) on algebraic coding theory. Last time we introduced linear codes and saw that they were easy to encode, but that we need extra structure to decode them. This time, we will construct a family of codes, called Reed-Solomon codes, which are optimal on large alphabets. Later on, we'll see that they are also efficiently decodable, so they're almost the best codes we can hope for.

(I originally planned to write about impossibility results, like the singleton and Plotkin bounds. However, I want to focus on other topics right now. Maybe I'll go back and fill in the gap later.)

## Rediscovering Reed-Solomon codes
When trying to digest a lovely piece of math, I often find it helpful to step back and wonder how anyone came up with their clever ideas in the first place. So, here's an attempt at rediscovering Reed-Solomon codes.

Recall that we'd like to find explicit linear codes with good distance. Since the distance of a linear code is also the minimum weight of its nonzero vectors, we're looking for a vector space where all the vectors have very few zero coordinates. If you do a lot of algebra, the thought of "very few zeros" might remind you of polynomials and their roots. We know that low-degree polynomials have few roots, and evaluating polynomials at their roots yield zeros, so...

*What if we made codewords by evaluating low-degree polynomials?* Suppose our messages are polynomials, and we map them to codewords by evaluating them at some points $$\alpha$$. If the polynomials have few roots, then evaluating them should give us vectors with only a few zeros, which is exactly what we want!

Of course, it would be much more difficult to come up with this idea if we didn't know anything about polynomials over finite fields. So I'll stop to review them for a bit.

## A detour into polynomials

You've probably learned about polynomials over $$\mathbb{R}$$ or $$\mathbb{C}$$. We can make polynomials over other fields too, and they work pretty much the way you'd expect.

>**Definition.** Let $$\mathbb{F}$$ be a field. A polynomial over $$\mathbb{F}$$ is a function $$f(x) = \sum a_i x^i$$ for some coefficients $$a_0, a_1, a_2... \in \mathbb{F}$$. The set of all polynomials over $$\mathbb{F}$$ is written $$\mathbb{F}[X]$$.

The degree of a polynomial $$\text{deg}(f)$$ is pretty much what you'd expect: it's the largest power with a nonzero coefficient. A root of a polynomial is a point $$\alpha$$ that evaluates to zero, $$f(\alpha) = 0$$. As mentioned previously, we care about polynomials because we wanted a code with mostly non-zero vectors, and *low-degree polynomials don't have too many roots.*

This is a highly important fact that gets used over and over in algebraic coding theory.

>**Proposition.** A nonzero polynomial of degree $$d$$ has at most $$d$$ roots.

>**Proof.** If the polynomial has a root, factor it out and do induction on the degree. $$\square$$

We also wanted a linear code, so I need to tell you what polynomials have to do with linear algebra. Actually, polynomials form a vector space! We can think of a polynomial $$f_{\boldsymbol{a}}(x) = \sum a_i x^i$$ as a vector of coefficients, $$\boldsymbol{a} = (a_0, a_1, ..., a_{k-1}) \in \mathbb{F}^k$$. We can add and scale these vectors by adding and scaling polynomials in the natural way. If $$f, g \in \mathbb{F}[X]$$ and $$\beta \in \mathbb{F}$$, then $$(f + g)(x) = f(x) + g(x)$$, and $$(\beta f)(x) = \beta f(x)$$.

Actually this definition is really nice, because it says that evaluation maps are linear. By evaluation map, I mean a map $$\phi: \boldsymbol{a} \to f_{\boldsymbol{a}}(\alpha)$$ which takes in polynomials $$\boldsymbol{a}$$ and evaluates them at point $$\alpha$$. Since evaluation maps are linear, we can represent them using matrices. These matrices might be useful, so let's give them a name:

>**Definition.** A Vandermonde matrix $$V$$ is a $$n \times k$$ matrix of the form
>$$V = \begin{pmatrix}
1      & \alpha_1 & ... & \alpha_1^{k-1} \\
\vdots & \vdots   & ... & \vdots \\
1      & \alpha_n & ... & \alpha_n^{k-1}
\end{pmatrix}$$

As promised, a Vandermonde matrix $$V$$ represents an evaluation map: $$V\boldsymbol{a} = (f_{\boldsymbol{a}}(\alpha_1), ..., f_{\boldsymbol{a}}(\alpha_n))$$. Since we want to decode messages using our codewords, we'd like to recover polynomials from their evaluations. Fortunately, this is possible:

>**Proposition.** Square Vandermonde matrices are invertible.

>**Proof.** By definition, $$V \boldsymbol{a} = (f_\boldsymbol{a}(\alpha_1), ..., f_\boldsymbol{a}(\alpha_n))$$, where $$\text{deg}(f_\boldsymbol{a}) < n$$. If $$a \neq 0$$, then $$f_\boldsymbol{a}$$ has fewer than $$n$$ roots. Hence, one of the $$\alpha_i$$ is not a root of $$f_\boldsymbol{a}$$, so $$V\boldsymbol{a} \neq 0$$. $$\square$$

>**Proposition.** Any square submatrix of a Vandermonde matrix is also invertible.

>**Proof.** Factor the submatrix into a diagonal matrix and a Vandermonde matrix, which are both invertible. $$\square$$

Actually, we can also correct errors. Intuitively, this is because polynomial interpolation works as usual: given a dataset of evaluated points, we can find a low-degree polynomial that passes through all of them.

>**Proposition (polynomial interpolation).** Given $$d$$ pairs $$(\alpha_i, y_i)$$ of points in $$\mathbb{F}$$, there is a unique polynomial $$f$$ of degree $$\leq d$$ passing through the pairs, $$f(\alpha_i) = y_i$$.

>**Proof.** Solve a linear system for the coefficients of $$f$$. This is possible because the linear system is a square submatrix of a Vandermonde matrix, which is invertible. $$\square$$

In fact, when we're working over a finite field $$\mathbb{F}_q$$, we can write down any function $$f: \mathbb{F}_q \to \mathbb{F}_q$$ by specifying its evaluations at all $$q$$ points. Hence, *every* such function $$f$$ is a polynomial!

## Back to RS codes

Armed with our understanding of low-degree polynomials and Vandermonde matrices, let's return to coding theory. Remember, we wanted a linear code which takes messages of length $$k$$ and maps them to codewords of length $$n$$ by evaluating a low-degree polynomial. Let's figure out how to do this.

* Since we want a linear code, the alphabet has to be a finite field $$\mathbb{F}_q$$.
* Also, we need to turn messages into polynomials somehow. We already saw that we can think of vectors and polynomials interchangeably -- we can interpret a message $$ \boldsymbol{a} = (a_0, ..., a_{k-1})$$ as the polynomial $$f_{\boldsymbol{a}}(x)  = \sum a_i x^i$$.
* Then we can evaluate the polynomial at $$n$$ different points $$\boldsymbol{\alpha} = (\alpha_1, ..., \alpha_n)$$, yielding a codeword $$(f_{\boldsymbol{a}}(\alpha_1), ..., f_{\boldsymbol{a}}(\alpha_n))$$.
* We can choose whatever evaluation points $$\boldsymbol{\alpha}$$ we want. We only require $$q \geq n$$ so that the points are all distinct.

Parsing through all that information, we arrive at the following code.

> **Definition.** Let $$q \geq n \geq k$$, and let $$\boldsymbol{\alpha} = \alpha_1, ..., \alpha_n \in \mathbb{F}_q$$ be distinct points. The Reed-Solomon (RS) code with block length $$n$$ and message length $$k$$ over $$\mathbb{F}_q$$ evaluated at $$\boldsymbol{\alpha}$$ is $$RS(n, k) = \{(f(\alpha_1), ..., f(\alpha_n)): f \in \mathbb{F}_q[X], \text{deg}(f) < k \}.$$

Given what we've seen so far, analyzing RS codes is easy! First, we designed them to be linear codes.

> **Proposition.** RS codes are linear codes.

>**Proof.** Polynomial evaluation is a linear map. In particular, Vandermonde matrices are generators for RS codes. $$\square$$

We motivated studying RS codes in the hopes that they would have pretty good distance. In fact, they are optimal!

> **Proposition.** An RS code $$RS(n, k)$$ has distance $$d = n-k+1$$.

> **Proof.** Since the polynomials have degree lower than $$k$$, they have fewer than $$k$$ roots. So the codewords each have fewer than $$k$$ zeroes, and the minimum weight of the code is at least $$n - k + 1$$. Since RS codes are linear, the weight is also the distance. By the Plotkin bound, this is optimal, so the distance is exactly $$n-k+1$$. $$\square$$

As mentioned previously, RS codes can also correct errors efficiently if we use a clever algorithm. Unfortunately, these marvellous codes come with one disadvantage: they require large alphabets. In fact, the alphabet size $$q$$ is at least the block length $$n$$, which is really unfortunate if we want to send longer messages. Since digital computers store data in bits, ideally we want a code that works with binary encodings, $$q=2$$. But that's a story for another day.

---
layout: post
title: "A bird's eye view of error correcting codes"
subtitle: "Algebraic coding theory, part 0"
thumbnail-img: /assets/img/message.png
share-img: /assets/img/message.png
tags: [math, coding theory, algebra]
---
I've recently taken an interest in coding theory, since it exhibits both beautiful mathematics and clear practical applications. Although I tried teaching myself the basics from *The Theory of Error-Correcting Codes* (MacWilliams and Sloane, 1977), the book is a bit old, and it's written more like an encyclopedia than a introductory text. Thankfully, Prof. Mary Wootters of Stanford recently released some Youtube [lectures](https://www.youtube.com/watch?v=vfjN7MmSB6g&list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr), which I find motivating and insightful. I'll post my thoughts here as I work through the series.

Whenever I learn a new subject, I like to get a high-level overview of the main concepts and how they fit together, before delving into the details. In this post, I give an overview of coding theory, as well as I understand it so far. I think of it mainly in three parts: the vanilla problem (basic theory), more flavors (extensions or variations of the theory), and applications. Let's dive in.

## The vanilla problem
Coding theory is all about how to communicate reliably and efficiently. Suppose Alice sends messages to Bob, but the message travels across a channel, where it can be slightly corrupted. To ensure reliable communication, Alice and Bob devise error-correcting codes. Alice sends some extra information, so that even after Bob receives a corrupted message, he can use the information to recover the original message.

![message](/assets/img/message.png){: .mx-auto.d-block :}

To study this problem more quantitatively, we model the world in which Alice and Bob send these messages. We'll start with the "vanilla" interpretation of this scenario: adversarial corruption, with full message recovery. Worst-case analysis is useful because algorithms that work well in the worst-case will also work well in practice.

* *What is a message?*  
  Alice and Bob write messages using an alphabet of $$q$$ letters. A message is a sequence of $$k$$ letters. (Computers represent data in bits, so most of the time $$q=2$$.) To add extra information to her message, Alice actually sends a codeword; this is a sequence of $$n$$ letters, where $$n \geq k$$.

* *How is a message corrupted?*  
  In the worst-case analysis, an eavesdropper Eve intercepts Alice's message and flips a few bits. Eve can choose the bits adversarially to foil Alice and Bob's coding scheme.

* *What is message recovery?*  
  Bob needs to recover all of Alice's original message, all of the time.

We would like to find codes that are reliable and efficient, in the following sense.

* *Distance*  
  Bob can recover Alice's messages. This is possible when the code has large distance -- that is, the codewords don't look too similar to each other.

* *Communication rate*  
  The codewords should not be too long. For instance, Alice can repeat her original message a million times, but Bob probably doesn't want to read all of that.

* *Efficiency*  
  Alice and Bob can figure out how to encode and decode messages in a reasonable amount of time.

Naturally there is no single perfect code. Instead, there are tradeoffs between the desired properties. In particular, distance and communication rate conflict with each other, because increasing the distance between the codewords decreases the number of codewords that can fit into the code. Coding theorists have found several impossibility results (the Hamming, Plotkin, and singleton bounds) which tell us which tradeoffs aren't achievable. On the flip side, there are some existential results (the GV bound) which tell us that some good codes exist, but don't give us an efficient way to compute them.

Finally, there are explicit results which describe codes that we can compute efficiently. While Reed-Solomon (RS) codes are optimal for large alphabets, computers run on binary encodings, so we need to modify them to work with smaller alphabets. This problem motivates several variations on RS codes, such as BCH and Reed-Muller codes. Concatenated codes, which apply two codes one after the other, can help convert between different alphabets.

## More flavors
While worst-case analysis can be useful, it can also be too pessimistic. We can try to design codes for different, possibly more realistic models of communication. Let's return to some of the questions we asked earlier.

* *How is the message corrupted?*  
  If Alice and Bob are secret agents expecting counterespionage, then they might expect adversarial corruption. However, if they work for NASA and send messages long distances through deep space, then the corruption is probably due to noisy interference, like cosmic rays. In this world, Bob can't recover Alice's message all of the time, but he only needs to recover it *most* of the time. That might lead him to discover things like channel capacity, LDPC codes, and polar codes.

* *What is message recovery?*  
  Originally, Bob wants to recover a message from Alice without any ambiguity. Unfortunately, this isn't possible when there are too many errors. Instead of decoding to get a single message, Bob could instead recover a short list of possible messages, and hopefully he would be able to infer Alice's true message based on other context. This is the topic of list decoding.

  In addition, Bob might be interested in bits and pieces of Alice's message. For instance, maybe Alice's message is extremely long and being stored on Amazon Cloud. If Bob is only interested in a small fragment of the message, he might want to decode a symbol by looking at symbols nearby, rather than the entire codeword. This problem motivates the topics of locally decodable and locally correctable codes. Alternatively, Bob might want to reconstruct the original codeword while looking at only a few of the received bits, leading to regenerating codes. This can be important in distributed storage, to correct corrupted data without making many slow requests across the network.

There are probably many more models than I'm listing here, mostly because I don't know very much about coding theory yet. For instance, I vaguely remember hearing something about insertion-deletion errors and iterative decoding. Maybe I'll check them out after I finish this lecture series.

## Applications, applications
Error-correcting codes have applications! Obviously, they are useful for storing and sending information. For instance, most DVDs use some kind of interleaved RS code to stay robust against dust and scratches, and NASA used polar coding on their [Voyager spacecraft](https://ntrs.nasa.gov/api/citations/20190032330/downloads/20190032330.pdf). It's also fairly easy to see how coding theory could be useful for cryptography, which is all about sending secret messages. But there are also applications in some other areas of algorithms and computer science, which I wasn't aware of before:

* *Compressed sensing.*  
  This is all about reconstructing signals as a sparse combination of basis signals, useful for imaging and computer vision. I vaguely remember reading some papers on the subject in college about how it was discovered by accident, when Tao and Donohue were able to recover medical images despite heavy noise. Compressed sensing recovers messages over field $$\mathbb{C}$$ while coding theory considers finite fields $$\mathbb{F}_{p^k}$$, but I guess many of the techniques are interchangeable.

* *Group testing.*  
  Suppose you know that there are a few sick people in a large population, and you can detect the disease using blood test. Instead of testing each person individually, it's far more efficient to arrange the people into different groups, then test each group with a single blood sample. The connection is pretty clear -- whereas group testing detects a few anomalies within a population with few tests, error correcting codes can correct a few errors using a few extra bits. So, we can use coding theory to help detect disease.

* *Heavy hitters.*  
  This is a topic from streaming and sketching algorithms. Suppose you run some company and customers place a giant stream of orders on your website. How can you efficiently track the most popular items without storing the entire stream of data?

In general, I find the idea of compressing and quicky retrieving information to be quite interesting -- isn't that why we have data structures and algorithms in the first place? I was at some point fascinated by geometric methods for data compression (e.g. random projects, metric embeddings), so coding theory seems to provide the algebraic counterpart to those topics.

Alright, that's a long enough summary of coding theory. I think I have enough of a bird's-eye view of the topic to begin learning it in earnest. Next time, let's begin to fill in the details.

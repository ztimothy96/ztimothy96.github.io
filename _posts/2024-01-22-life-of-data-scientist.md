---
layout: post
title: "Life of a data scientist"
thumbnail-img: /assets/img/social-graph.png
share-img: /assets/img/social-graph.png
tags: [data science]
---
It's been some time since I started my gig as a junior data scientist, and I like it a fair amount, so I thought I'd write about it. I can say some things about what I've learned from my experience so far, what a typical day is like, and so on. I don't think my current gig is fully representative of what a data scientist might do at a higher level, but it's already pretty different from what I've done before.

A long time ago, I knew some software engineers who'd laugh at the sloppy, hacky code of data scientists at their company and remark, "That doesn't look like proper software engineering!" And they were completely right: data science *isn't* software engineering. Well, not quite.

While both roles require a good deal of computer programming, the underlying goals differ considerably. In software engineering, the software is the product; the engineer aims to produce a reliable, functional app that people can use from all over the world. However, for the data scientist, the software is a means to an end, namely producing *insight* from data. We use software to generate data, visualize data, process data, model data, make predictions about data, etc., but we care more about the value extracted from data, rather than the software itself. As a result, code quality standards are not quite as high.

This isn't to say that data scientists don't need to produce high-quality code, or that they're not capable of doing so. But there are trade-offs. Writing extremely high-quality, performant, easily maintainable code takes time and effort, and often producing such code might not be worth the cost. For instance, data scientists may spend a large amount of time visualizing data and making exploratory hypotheses. Once we have found some patterns, we may not need to maintain the visualization script. This code will not be shipped out for use by millions of customers worldwide; in fact, we might never use it again. We may want it to look clean enough that a third party could reproduce the work, but we probably don't need to polish it to the degree of, say, the YouTube backend infrastructure.

Instead of over-optimizing on writing pretty programs, we might instead spend more time interpreting the data, which would better help achieve the company's goals. For the sake of exposition, suppose that I work at a farm trying to optimize yields. (I don't, but the project is analogous...) Last year I trained some neural networks to predict how quickly plants would grow, but a scatterplot showed that my model was biased towards underpredicting. I noticed that the data looked bimodal, forming two tight clusters. Why might that happen?

Investigating further, I discovered that the data distribution had *shifted* over time! The farmers had recently decided to grow different species of plants, which were on average larger than those in previous years. Since the model had trained only on smaller varieties of plants, it failed to predict the behavior of the new species. Once I eliminated this shift (e.g. normalizing by average plant size per species), the problem went away. That might give a good example of how a data scientist should ask questions.

Frequently, answering these questions will require visualization and exploratory scripting. For instance, we might test the growth rate of various plants under different temperatures and humidity. Which variables would impact the growth rate more? If we found that changing the temperature created drastic variations in growth patterns, how should we account for that in our model? Maybe we could simply feed temperature into our model as a feature, or maybe we might need to add its interaction with other features. Maybe we should create a *different* model for each temperature under test, who knows? There's only one way to find out for sure: test everything. A data scientist might end up writing heaps of experimental code that never makes it into the final working model. We just want to get some results quickly and move on.

The need for quick, hacky, exploratory analysis has shaped the data science workflow and toolkit. I used to write everything in vanilla Python scripts, but I increasingly find myself using Jupyter notebooks. The ability to run select portions of a program speeds up the iteration process enormously. (This is especially true when you have a somewhat sizeable dataset and don't want to waste time loading it repeatedly). I'm also discovering the importance of keeping up-to-date with new libraries that can simplify workflow and help me focus on data rather than programming; we might use [Streamlit](https://streamlit.io/) to avoid writing frontend apps, or we might import [seaborn](https://seaborn.pydata.org/) instead of matplotlib for its high-level visualization API. Sure, we lose a bit of customizability, but unless we need to produce top-notch graphics to impress people at an important meeting, it will suffice.

Perhaps unfortunately, much of a data scientist's work consists of cleaning, preprocessing, and visualizing data. That's fine; I knew this before I signed up for the gig, and I don't mind the preprocessing as long as there are enough new skills to learn and interesting questions to ponder.

One last thought: I made a new friend at Google who also works on some data science projects, and he says that getting a math Ph.D. was total overkill for his career. In daily life, he only needs to know high-school statistics. I have much the same experience (except that I didn't bother to finish my degree) and don't understand why companies ask for these stringent qualifications. But still, high school statistics is better than no statistics, isn't it? And besides, data visualizations can look super pretty sometimes. Enjoy this graphic of social networks on Facebook.

![graph](/assets/img/social-graph.png){: .mx-auto.d-block :}

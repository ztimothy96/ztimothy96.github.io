---
layout: post
title: "Monkeying around on Independence Day"
subtitle: "Some facts about probability and independence"
thumbnail-img:  /assets/img/binomial-normal-apx.jpg
share-img: /assets/img/binomial-normal-apx.jpg
tags: [math, probability]
---

It's Independence Day, and I've been meaning to review my probability theory notes for a while now, so it's a perfect time to write about independent random variables! In this post I'll talk about the basic results involving independence, like the infinite monkey theorem, Kolmogorov's $$0-1$$ law, and the Borel-Cantelli lemma.

## Monkeys and tails
Most of the classical theorems of probability theory (namely the Law of Large Numbers and the Central Limit Theorem) are about infinite sequences of independent variables. But instead of delving into all that serious stuff, let's spend some time monkeying around.

>**Question.** Suppose that a monkey sits at a standard English typewriter, typing out letters randomly and independently.
> - What is the probability that the monkey will type out Shakespeare's *Hamlet* at some point?
> - What is the probability that the monkey will type out *Hamlet* infinitely many times?
> - What is the probability that the monkey will eventually only type out $$42$$-digit strings from the decimal expansion of $$\pi$$?

![clt](/assets/img/binomial-normal-apx.jpg){: .mx-auto.d-block :}

> The Central Limit Theorem, the most useful fact about independent variables. Not as fun as random monkeys, though.

Let's begin with the last two questions, which ask about events that only depend on what the monkey types out *eventually*. Even if we tossed out the first $$100$$ letters typed by the monkey, we can still observe whether the events happen. More formally, recall that a $$\sigma$$-field is a collection of events whose probability we can measure; we want to collect only those events that depend on the tail of the sequence.

>**Definition.** Let $$(X_n)_{n \in \mathbb{N}}$$ be a sequence of random variables, and let $$\mathcal{F}_n = \sigma(X_n, X_{n+1}, ...)$$ be the $$\sigma$$-field of events generated by variables $$(X_k)_{k \geq n}$$. The *tail $$\sigma$$-field* of the sequence is $$\mathcal{T} = \bigcap_n \mathcal{F}_n$$. Elements of $$\mathcal{T}$$ are called *tail events*.

The tail $$\sigma$$-field contains the events that you could observe even if you toss away any finite number of variables $$X_n$$. Since tail events are independent of the first $$n$$ variables, for any $$n < \infty$$, they only depend on the long-term behavior of the sequence. Perhaps surprisingly, a tail event either always happens, or never happens at all.

>**Theorem (Kolmogorov $$0-1$$ law).**
Let $$(X_n)_{n \in \mathbb{N}}$$ be independent random variables, and let $$\mathcal{T}$$ be their tail $$\sigma$$-field. Then for all events $$E \in \mathcal{T}$$, $$\Pr(E) \in \{ 0, 1\}$$.

>**Proof.**
By definition, $$\mathcal{T}$$ is independent of the all the events generated by the first $$n$$ variables $$\sigma(X_1, ..., X_n)$$. Since $$\mathcal{T}$$ is independent of $$\sigma$$-fields generated by any finite number of variables, it is also independent of $$\lim_{n \to \infty} \sigma(X_1, ..., X_n) = \mathcal{F}$$, the $$\sigma$$-field of all events! In particular, $$\mathcal{T} \subseteq \mathcal{F}$$, so $$\mathcal{T}$$ must be independent *of itself*. Then for each tail event $$E$$, $$\Pr(E) = \Pr(E \cap E) = \Pr(E)^2$$. We conclude that $$\Pr(E) \in \{0, 1\}$$. $$\square$$

The Kolmogorov $$0-1$$ law is interesting for a few reasons. For the philosopher, it sounds a lot like predestination: events at the end of time either happen or they don't, and there's nothing you can do about it. For the mathematician, the theorem is an incredibly overpowered tool, as it tells us a lot about all tail probabilities, even if we have no idea about how to actually compute them. If we want to show that a tail event $$E$$ almost surely happens, we just have to prove that $$\Pr(E) > 0$$! As an example of how ridiculous this theorem can be, we can immediately conclude the following.

>**Corollary.**
Suppose that a monkey sits at a standard English typewriter, typing out letters randomly and independently.
>
>- The probability that the monkey will type out *Hamlet* infinitely many times is $$0$$ or $$1$$.
>- The probability that the monkey will eventually only type out $$42$$-digit strings from the decimal expansion of $$\pi$$ is $$0$$ or $$1$$.

So that's all well and good, but are the probabilities $$0$$ or $$1$$? To answer that question, we need to develop some techniques.

## The Borel-Cantelli lemma

Let's start by answering the easiest question about our prolific monkey friend.
>**Theorem (infinite monkeys).**
Suppose that a monkey sits at a standard English typewriter, typing out letters randomly and independently. Then the monkey will eventually type out Shakespeare's *Hamlet* with probability $$1$$.

>**Proof.**
Let $$k$$ be the number of unique letters on the keyboard, and let $$n$$ be the number of letters in *Hamlet*. By independence, the probability of typing out *Hamlet* starting at the first character is $$\frac{1}{k^n} > 0$$. By independence again, the probability of typing out *Hamlet* starting at the $$(n+1)$$-th, $$(2n+1)$$-th, or $$(3n+1)$$-th typed letter is the same, and so on. Then the probability of not typing out *Hamlet* after $$t$$ tries is $$(1 - \frac{1}{k^n})^t \to 0$$ as $$t \to \infty$$. $$\square$$

This result has been the subject of many debates about science and philosophy, revolving around whether order can come from chaos. Although the monkey seems to suggest that randomness can produce nearly everything interesting, in practice it would take a ridiculous amount of time before the monkey wrote even a single page of *Hamlet*. Nonetheless, the infinite monkey theorem has also inspired research into random text generation. The practicality of infinitely typing monkeys aside, a slightly generalized version of the theorem is broadly useful across probability theory.

In what follows, "i.o." stands for "infinitely often."

>**Lemma (Borel-Cantelli).**
Let $$(E_n)_{n \in \mathbb{N}}$$ be a sequence of events. If $$\sum_n \Pr(E_n) < \infty$$, then $$\Pr(E_n \, \text{i.o.}) = 0$$. Conversely, if $$\sum_n \Pr(E_n) = \infty$$ and $$(E_n)$$ is independent, then $$\Pr(E_n \, \text{i.o.}) = 1$$.

This result is extremely convenient, because now we can figure out whether events occur infinitely often just by summing together some probabilities. In particular, it gives us an easy way to prove almost-sure convergence of random variables (which I might write about in a future post).

>**Proof.**
Suppose that $$\sum_n \Pr(E_n) < \infty$$. Then $$\sum_{k=n}^\infty E_k \to 0$$ as $$n \to \infty$$, and
$$
\begin{align}
\Pr(E_n \, \text{i.o.}) &= \Pr(\lim \sup E_n) \\
&= \Pr \left(\lim_{n \to \infty} \bigcup_{k \geq n} E_k \right) \\
&= \lim_{n \to \infty} \Pr \left(\bigcup_{k \geq n} E_k \right) \\
&\leq \lim_{n \to \infty} \sum_{k \geq n} E_k = 0.
\end{align}
$$

>For the partial converse, suppose that $$\sum \Pr(E_n) = \infty$$ and that the $$(E_n)$$ are independent. Proceeding as before and using independence,
$$
\begin{align}
\Pr(\{E_n \, \text{i.o.}\}^c) &= \Pr \left( \lim \inf E_n^c \right) \\
&= \lim_{n \to \infty} \Pr \left( \bigcap_{k \geq n} E_k^c \right) \\
&= \lim_{n \to \infty} \prod_{k \geq n} (1 - \Pr(E_k)).
\end{align}
$$
>
>By the helpful inequality $$1 - x \leq e^{-x}$$,
$$\lim_{n \to \infty} \prod_{k \geq n} (1 - \Pr(E_k)) \leq \lim_{n \to \infty} \exp \left(-\sum_{k = n} \Pr(E_k) \right) \leq \exp (-\infty) = 0. \; \square$$

To see why the Borel-Cantelli lemma generalizes the infinite monkey theorem, let's prove the infinite monkey theorem again.

>**Corollary.**
With probability $$1$$, the monkey types out *Hamlet* infinitely many times.

>**Proof.**
Let $$k, n$$ be defined as before, and let $$E_t$$ be the event of typing out *Hamlet* starting from the $$(nt+1)$$-th letter. Each such event has probability $$\frac{1}{k^n} > 0$$, as before. Since $$\sum_t \Pr(E_t) = \infty$$, and all the events are independent, the Borel-Cantelli lemma tells us that the monkey will type out *Hamlet* infinitely many times. $$\square$$

Finally, let's examine the last question we posed earlier, about whether the monkey will eventually only type $$42$$-digit substrings from the decimal expansion of $$\pi$$.

>**Definition.**
A number $$x \in [0, 1]$$ is *normal* if its decimal expansion contains every possible finite-length substring using the digits $$\{0, ..., 9\}$$.

>**Theorem.**
The infinitely typing monkey will produce a normal number with probability $$1$$.

>**Proof.**
Consider any finite string. The monkey eventually types out this string with probability $$1$$, or equivalently, it avoids the string with probability $$0$$. Since there are countably many finite strings, let's enumerate them, and let $$E_n$$ be the event that the monkey avoids the $$n$$-th string. The probability of avoiding any string is then $$\Pr(\bigcup_n E_n) \leq \sum_n \Pr(E_n) = 0$$, so the monkey will eventually type every finite string. $$\square$$

In particular, the monkey will type out every possible $$42$$-digit string infinitely many times. Consequently, it will only type out strings from $$\pi$$ if and only if $$\pi$$ itself contains every possible $$42$$-digit string in its expansion. As far as I am aware, humanity has managed to compute $$\pi$$ up to roughly $$10^{13} << 10^{42}$$ digits, so we don't actually know whether $$\pi$$ contains all possible $$42$$-digit strings. We're back where we started; the probability is either $$0$$ or $$1$$, and it's unlikely we'll know what the answer is until somebody discovers more math.
